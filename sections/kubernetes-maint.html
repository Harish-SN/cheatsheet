<h2>Kubernetes Maintenance</h2>
<p>Cluster upkeep: upgrades, draining, scaling, zero-downtime website updates, maintenance windows, troubleshooting,
    certificates, and backups.</p>
<div class="grid">

    <!-- Node draining & cordon -->
    <details open>
        <summary>Node Maintenance (Drain / Cordon / Uncordon)</summary>
        <div class="code">
            <button class="copy" data-target="k8s-node">Copy</button>
            <pre id="k8s-node"><code># Cordon (mark unschedulable)
kubectl cordon node-1

# Drain (evict pods, ignore DaemonSets)
kubectl drain node-1 --ignore-daemonsets --delete-emptydir-data

# Uncordon (bring node back)
kubectl uncordon node-1
</code></pre>
        </div>
    </details>

    <!-- Cluster upgrades -->
    <details>
        <summary>Cluster Upgrade (kubeadm)</summary>
        <div class="code">
            <button class="copy" data-target="k8s-upgrade">Copy</button>
            <pre id="k8s-upgrade"><code># Check upgrade versions
kubeadm upgrade plan

# Upgrade control plane (master node)
sudo apt-get update && sudo apt-get install -y kubeadm=1.30.x-00
sudo kubeadm upgrade apply v1.30.x

# Upgrade kubelet & kubectl
sudo apt-get install -y kubelet=1.30.x-00 kubectl=1.30.x-00
sudo systemctl daemon-reexec && sudo systemctl restart kubelet

# Upgrade worker nodes
kubectl drain node-1 --ignore-daemonsets --delete-emptydir-data
sudo apt-get install -y kubeadm=1.30.x-00
sudo kubeadm upgrade node
sudo apt-get install -y kubelet=1.30.x-00 kubectl=1.30.x-00
sudo systemctl restart kubelet
kubectl uncordon node-1
</code></pre>
        </div>
    </details>

    <!-- Website updates: zero-downtime -->
    <details open>
        <summary>Website Update (Zero-Downtime: Rolling / Canary / Blue-Green)</summary>

        <!-- Rolling -->
        <div class="code">
            <div class="card-head">Rolling Update (Recommended)</div>
            <button class="copy" data-target="k8s-rolling">Copy</button>
            <pre id="k8s-rolling"><code># Ensure Deployment uses safe strategy + probes
cat <<'YAML' | kubectl apply -f -
apiVersion: apps/v1
kind: Deployment
metadata: { name: web, namespace: default }
spec:
  replicas: 3
  strategy:
    type: RollingUpdate
    rollingUpdate:
      maxSurge: 1
      maxUnavailable: 0
  selector: { matchLabels: { app: web } }
  template:
    metadata: { labels: { app: web } }
    spec:
      containers:
      - name: web
        image: nginx:1.25.5
        ports: [{ containerPort: 80 }]
        readinessProbe:
          httpGet: { path: /, port: 80 }
          initialDelaySeconds: 5
          periodSeconds: 5
        livenessProbe:
          httpGet: { path: /, port: 80 }
          initialDelaySeconds: 10
          periodSeconds: 10
YAML

# Update image (triggers rolling update)
kubectl set image deploy/web web=nginx:1.26.0

# Watch rollout, verify pods/endpoints
kubectl rollout status deploy/web
kubectl get pods -l app=web -o wide
kubectl get endpoints web
</code></pre>
        </div>

        <!-- Canary -->
        <div class="code">
            <div class="card-head">Canary Release (NGINX Ingress)</div>
            <button class="copy" data-target="k8s-canary">Copy</button>
            <pre id="k8s-canary"><code># Create canary deployment/service
kubectl create deploy web-canary --image=nginx:1.26.0 --replicas=1
kubectl expose deploy web-canary --port 80 --target-port 80 --name web-canary

# Route 10% traffic to canary (Ingress annotations)
cat <<'YAML' | kubectl apply -f -
apiVersion: networking.k8s.io/v1
kind: Ingress
metadata:
  name: web
  annotations:
    kubernetes.io/ingress.class: nginx
    nginx.ingress.kubernetes.io/canary: "true"
    nginx.ingress.kubernetes.io/canary-weight: "10"
spec:
  rules:
  - host: example.com
    http:
      paths:
      - path: /
        pathType: Prefix
        backend: { service: { name: web-canary, port: { number: 80 } } }
YAML

# Increase gradually to 25%, then 50%
kubectl annotate ingress web nginx.ingress.kubernetes.io/canary-weight="25" --overwrite
kubectl annotate ingress web nginx.ingress.kubernetes.io/canary-weight="50" --overwrite

# Promote to 100% (update stable) & remove canary
kubectl set image deploy/web web=nginx:1.26.0
kubectl scale deploy/web-canary --replicas=0
kubectl annotate ingress web nginx.ingress.kubernetes.io/canary- --overwrite
</code></pre>
        </div>

        <!-- Blue/Green -->
        <div class="code">
            <div class="card-head">Blue-Green (Instant Switch)</div>
            <button class="copy" data-target="k8s-bluegreen">Copy</button>
            <pre id="k8s-bluegreen"><code># Two deployments: web-blue (live), web-green (new)
kubectl create deploy web-green --image=nginx:1.26.0 --replicas=3
kubectl label deploy web-blue color=blue --overwrite
kubectl label deploy web-green color=green --overwrite

# Service initially points to blue
cat <<'YAML' | kubectl apply -f -
apiVersion: v1
kind: Service
metadata: { name: web }
spec:
  selector: { app: web, color: blue }
  ports: [{ name: http, port: 80, targetPort: 80 }]
YAML

# Switch Service to green when ready
kubectl patch svc web --type='merge' -p '{"spec":{"selector":{"app":"web","color":"green"}}}'

# Rollback fast by pointing back to blue
kubectl patch svc web --type='merge' -p '{"spec":{"selector":{"app":"web","color":"blue"}}}'

# After confidence window, scale down old
kubectl scale deploy/web-blue --replicas=0
</code></pre>
        </div>

        <!-- Hardening -->
        <div class="code">
            <div class="card-head">Hardening: PDB & Surge</div>
            <button class="copy" data-target="k8s-hardening">Copy</button>
            <pre id="k8s-hardening"><code># Ensure at least 2 pods stay up during drains/updates
cat <<'YAML' | kubectl apply -f -
apiVersion: policy/v1
kind: PodDisruptionBudget
metadata: { name: web-pdb }
spec:
  minAvailable: 2
  selector: { matchLabels: { app: web } }
YAML

# Deployment strategy should keep:
# - maxUnavailable: 0
# - maxSurge: >= 1
# - readinessProbe defined (mandatory), livenessProbe recommended.
</code></pre>
        </div>

    </details>

    <!-- Maintenance window / splash -->
    <details>
        <summary>Maintenance Window (Splash / 503)</summary>
        <div class="code">
            <button class="copy" data-target="k8s-maint-mode">Copy</button>
            <pre id="k8s-maint-mode"><code># Option A: swap Service selector to a maintenance deployment
kubectl create deploy web-maint --image=nginx:alpine --replicas=2
# (Serve your static maintenance page via ConfigMap or baked into the image)

# Point Service to maint temporarily
kubectl patch svc web --type='merge' -p '{"spec":{"selector":{"app":"web-maint"}}}'

# Revert after work
kubectl patch svc web --type='merge' -p '{"spec":{"selector":{"app":"web"}}}'
kubectl scale deploy/web-maint --replicas=0

# Option B: patch Ingress backend to maintenance service (instant cutover)
kubectl patch ingress web --type='json' -p='[
  {"op":"replace","path":"/spec/rules/0/http/paths/0/backend/service/name","value":"web-maint"}
]'
# Revert by switching back to "web" service name.

# Quick checks
kubectl get endpoints web -o wide
kubectl get ingress web -o yaml | yq '.spec.rules[0].http.paths[0].backend.service.name'
</code></pre>
        </div>
    </details>

    <!-- Scaling -->
    <details>
        <summary>Scaling & Autohealing</summary>
        <div class="code">
            <button class="copy" data-target="k8s-scale">Copy</button>
            <pre id="k8s-scale"><code># Manually scale deployment
kubectl scale deployment my-app --replicas=5

# Horizontal Pod Autoscaler (HPA)
kubectl autoscale deployment my-app --cpu-percent=70 --min=2 --max=10

# View HPA status
kubectl get hpa
</code></pre>
        </div>
    </details>

    <!-- Logs & troubleshooting -->
    <details>
        <summary>Troubleshooting</summary>
        <div class="code">
            <button class="copy" data-target="k8s-troubleshoot">Copy</button>
            <pre id="k8s-troubleshoot"><code># Pods
kubectl describe pod my-app-12345
kubectl logs my-app-12345 -f

# Events
kubectl get events --sort-by=.metadata.creationTimestamp

# Check node status
kubectl get nodes -o wide

# Debug inside a pod
kubectl exec -it my-app-12345 -- /bin/sh

# Ephemeral debug container (target another container)
kubectl debug pod/my-app-12345 -it --image=busybox:1.36 --target=my-container
</code></pre>
        </div>
    </details>

    <!-- Certificates -->
    <details>
        <summary>Certificates & Expiry</summary>
        <div class="code">
            <button class="copy" data-target="k8s-certs">Copy</button>
            <pre id="k8s-certs"><code># Check kubeadm certs expiry
kubeadm certs check-expiration

# Renew all control-plane certs
sudo kubeadm certs renew all

# Restart control plane pods (if static pods, they'll auto-restart)
sudo systemctl restart kubelet
</code></pre>
        </div>
    </details>

    <!-- Backups -->
    <details>
        <summary>Backups (etcd & manifests)</summary>
        <div class="code">
            <button class="copy" data-target="k8s-backup">Copy</button>
            <pre id="k8s-backup"><code># Backup etcd (on control plane)
ETCDCTL_API=3 etcdctl snapshot save snapshot.db \
  --endpoints=https://127.0.0.1:2379 \
  --cacert=/etc/kubernetes/pki/etcd/ca.crt \
  --cert=/etc/kubernetes/pki/etcd/server.crt \
  --key=/etc/kubernetes/pki/etcd/server.key

# Restore etcd snapshot
ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \
  --data-dir /var/lib/etcd-from-backup

# Backup manifests
cp -r /etc/kubernetes/manifests /root/manifests-backup-$(date +%F)
</code></pre>
        </div>
    </details>

</div>